{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Tests\n",
    "for LiftOn, Liftoff, and miniprot\n",
    "- on CHM13_MANE and CHM13_RefSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to the directory `/ccb/salz2/kh.chao/Lifton/results/` to access the LiftOn results. Within this directory, you’ll find subdirectories for `bee`, `rice`, `arabadop` (arabidopsis), `human_mane`, and `human_chess`\n",
    "Each subdirectory contains a `score.txt` file with columns for “transcript ID,” “Liftoff protein sequence identity,” “miniprot protein sequence identity,” “LiftOn protein sequence identity,” “Category,” “Mutation types,” and “gene_locus.”\n",
    "Here are three things we want to check first:\n",
    "- LiftOn score is better than Liftoff & miniprot\n",
    "- How many transcripts have the same score as Liftoff; how many transcripts have the same score as miniprot (this category means that LiftOn directly uses either Liftoff or miniprot’s CDSs)\n",
    "- LiftOn score is worse than Liftoff & miniprot\n",
    "\n",
    "For 1 and 3, you can use the automated igv.sh script to visualize the results on IGV. Specifically:\n",
    "- For 1, we want to see how exactly LiftOn chains the CDSs and provides an improved annotation (and how many cases)\n",
    "- For 3, examine why LiftOn may have inaccurately annotated. This helps us to improve the current version of LiftOn\n",
    "\n",
    "The sequence identity dot plots and frequency plots are here: `/ccb/salz2/kh.chao/Lifton/results/arabadop/visualization/`.\n",
    "In the identity dot plots, you can see there are still some dots that are on the bottom right panel. We want to fix those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "OUTPUT_DIR = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect scores from original score files\n",
    "referencing kh's code from `ccb/salz2/kh.chao/Lifton/lifton/lifton_utils.py` -> `write_lifton_status()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_lifton_status(fw_score, transcript_id, transcript, lifton_status):\n",
    "#     final_status = \";\".join(lifton_status.status)\n",
    "#     fw_score.write(f\"{transcript_id}\\t{lifton_status.liftoff}\\t{lifton_status.miniprot}\\t{lifton_status.lifton}\\t{lifton_status.annotation}\\t{final_status}\\t{transcript.seqid}:{transcript.start}-{transcript.end}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate stats from score comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_score(score_file, verbose=False):\n",
    "    # read the given species file into a dataframe\n",
    "    column_names = ['transcript_id', 'liftoff_score', 'miniprot_score', 'lifton_score', 'category', 'mutation_type', 'gene_locus']\n",
    "    df = pd.read_csv(score_file,  names=column_names, delimiter='\\t')\n",
    "    if verbose:\n",
    "        print(f'Reading in {score_file}...')\n",
    "        print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''!!!!!!WARNING: DELETING DIRECTORIES!!!!!!!'''\n",
    "def delete_directory(directory_path):\n",
    "    if os.path.exists(directory_path):\n",
    "        # The directory exists, so delete it\n",
    "        try:\n",
    "            shutil.rmtree(directory_path)\n",
    "            print(f\"Directory '{directory_path}' deleted successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting directory '{directory_path}': {e}\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory_path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stats(df, species, clean=False, verbose=False):\n",
    "    # get filenames\n",
    "    if verbose: \n",
    "        print(f'Collecting stats for {species}...')\n",
    "    \n",
    "    if clean:\n",
    "        delete_directory(f'{OUTPUT_DIR}/{species}/')\n",
    "        \n",
    "    os.makedirs(f'{OUTPUT_DIR}/{species}/', exist_ok=True)\n",
    "    lifton_better_filepath = f'{OUTPUT_DIR}/{species}/lifton_better.out'\n",
    "    lifton_worse_filepath = f'{OUTPUT_DIR}/{species}/lifton_worse.out'\n",
    "    stats_filepath = f'{OUTPUT_DIR}/{species}/score_stats.out'\n",
    "    \n",
    "    # collect statistics within dataframe\n",
    "    for col1 in df.columns[1:4]:\n",
    "        for col2 in df.columns[1:4]:\n",
    "            if col1 != col2:\n",
    "                is_greater = df[col1] > df[col2]\n",
    "                is_equal = df[col1] == df[col2]\n",
    "\n",
    "                df[f'{col1}>{col2}'] = is_greater\n",
    "                df[f'{col1}={col2}'] = is_equal\n",
    "    \n",
    "    stats_counts = df.iloc[:, -12:].sum()\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Collected stat counts:\\n{stats_counts}\\n')\n",
    "    \n",
    "    # write selected scenarios to file\n",
    "    out = '-------------- SCENARIOS ------------\\n'\n",
    "    ## 1. lifton best\n",
    "    lifton_best = df[df['lifton_score>liftoff_score'] & df['lifton_score>miniprot_score']].iloc[:, :-12]\n",
    "    lifton_best.to_csv(lifton_better_filepath, index=False, header=False, sep='\\t')\n",
    "    out += f'LiftOn best\\t\\t\\t\\t\\t\\t{len(lifton_best)}\\n'\n",
    "    ## 2. liftoff best\n",
    "    liftoff_best = df[df['liftoff_score>lifton_score'] & df['liftoff_score>miniprot_score']]\n",
    "    out += f'Liftoff best\\t\\t\\t\\t\\t{len(liftoff_best)}\\n'\n",
    "    ## 3. miniprot best\n",
    "    miniprot_best = df[df['miniprot_score>lifton_score'] & df['miniprot_score>liftoff_score']]\n",
    "    out += f'miniprot best\\t\\t\\t\\t\\t{len(miniprot_best)}\\n'\n",
    "    ## 4. lifton < liftoff only\n",
    "    lifton_worse_liftoff = df[df['liftoff_score>lifton_score'] & ~df['miniprot_score>lifton_score']].iloc[:, :-12]\n",
    "    lifton_worse_liftoff.to_csv(lifton_worse_filepath, index=False, header=False, sep='\\t')\n",
    "    out += f'LiftOn worse than Liftoff only\\t{len(lifton_worse_liftoff)}\\n' \n",
    "    ## 5. lifton < miniprot only\n",
    "    lifton_worse_miniprot = df[df['miniprot_score>lifton_score'] & ~df['liftoff_score>lifton_score']].iloc[:, :-12]\n",
    "    lifton_worse_miniprot.to_csv(lifton_worse_filepath, mode='a', index=False, header=False, sep='\\t')\n",
    "    out += f'LiftOn worse than miniprot only\\t{len(lifton_worse_miniprot)}\\n' \n",
    "    ## 6. lifton worst\n",
    "    lifton_worst = df[df['liftoff_score>lifton_score'] & df['miniprot_score>lifton_score']].iloc[:, :-12]\n",
    "    lifton_worst.to_csv(lifton_worse_filepath, mode='a', index=False, header=False, sep='\\t')\n",
    "    out += f'LiftOn worst\\t\\t\\t\\t\\t{len(lifton_worst)}\\n' \n",
    "    ## 7. lifton == liftoff only\n",
    "    lifton_eq_liftoff = df[df['lifton_score=liftoff_score'] & ~df['lifton_score=miniprot_score']]\n",
    "    out += f'LiftOn equal to Liftoff only\\t{len(lifton_eq_liftoff)}\\n'\n",
    "    ## 8. lifton == miniprot only \n",
    "    lifton_eq_miniprot = df[df['lifton_score=miniprot_score'] & ~df['lifton_score=liftoff_score']]\n",
    "    out += f'LiftOn equal to miniprot only\\t{len(lifton_eq_miniprot)}\\n'\n",
    "    ## 9. all equal\n",
    "    all_eq = df[df['lifton_score=miniprot_score'] & df['lifton_score=liftoff_score']]\n",
    "    out += f'All equal\\t\\t\\t\\t\\t\\t{len(all_eq)}\\n'\n",
    "    ## 10. total counted\n",
    "    out += f'Total counts\\t\\t\\t\\t\\t{len(df)}\\n'\n",
    "\n",
    "    # do all mutually exclusive cases, ranked\n",
    "    out += '\\n----------------- ALL ---------------\\n'\n",
    "    out += f\"lifton=liftoff=miniprot\\t{len(all_eq)}\\n\"\n",
    "    out += f\"lifton>liftoff=miniprot\\t{len(df[df['lifton_score>liftoff_score'] & df['liftoff_score=miniprot_score']])}\\n\"\n",
    "    out += f\"lifton>liftoff>miniprot\\t{len(df[df['lifton_score>liftoff_score'] & df['liftoff_score>miniprot_score']])}\\n\"\n",
    "    out += f\"lifton>miniprot>liftoff\\t{len(df[df['lifton_score>miniprot_score'] & df['miniprot_score>liftoff_score']])}\\n\"\n",
    "    out += f\"lifton=liftoff>miniprot\\t{len(df[df['lifton_score=liftoff_score'] & df['lifton_score>miniprot_score']])}\\n\"\n",
    "    out += f\"lifton=miniprot>liftoff\\t{len(df[df['lifton_score=miniprot_score'] & df['lifton_score>liftoff_score']])}\\n\"\n",
    "    out += f\"liftoff>lifton>miniprot\\t{len(df[df['liftoff_score>lifton_score'] & df['lifton_score>miniprot_score']])}\\n\"\n",
    "    out += f\"miniprot>lifton>liftoff\\t{len(df[df['miniprot_score>lifton_score'] & df['lifton_score>liftoff_score']])}\\n\"\n",
    "    out += f\"liftoff>lifton=miniprot\\t{len(df[df['liftoff_score>lifton_score'] & df['lifton_score=miniprot_score']])}\\n\"\n",
    "    out += f\"miniprot>lifton=liftoff\\t{len(df[df['miniprot_score>lifton_score'] & df['lifton_score=liftoff_score']])}\\n\"\n",
    "    out += f\"liftoff=miniprot>lifton\\t{len(df[df['liftoff_score=miniprot_score'] & df['liftoff_score>lifton_score']])}\\n\"\n",
    "    out += f\"liftoff>miniprot>lifton\\t{len(df[df['liftoff_score>miniprot_score'] & df['miniprot_score>lifton_score']])}\\n\"\n",
    "    out += f\"miniprot>liftoff>lifton\\t{len(df[df['miniprot_score>liftoff_score'] & df['liftoff_score>lifton_score']])}\\n\"\n",
    "    out += f\"all\\t\\t\\t\\t\\t\\t{len(df)}\\n\"\n",
    "\n",
    "    if verbose: \n",
    "        print(out)\n",
    "    \n",
    "    # adding raw data to the file\n",
    "    with open(stats_filepath, 'a') as f:\n",
    "        f.write(out)\n",
    "        f.write('\\n----------------- RAW ---------------\\n')\n",
    "    stats_counts.to_csv(stats_filepath, mode='a', header=False, sep='\\t')\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Wrote stat results to: {stats_filepath}, {lifton_better_filepath}, {lifton_worse_filepath}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_stats(species_list, output_path, verbose=False):\n",
    "\n",
    "    # read in each individual stats_file and generate an aggregate file\n",
    "    if verbose:\n",
    "        print(f'Aggregating stats for {list(species_list)}...')\n",
    "\n",
    "    data_list = []\n",
    "    for species in species_list:\n",
    "        \n",
    "        # define file\n",
    "        filepath = f'{OUTPUT_DIR}/{species}/score_stats.out'\n",
    "        if verbose:\n",
    "            print(f'Parsing {filepath}...')\n",
    "    \n",
    "        # read the lines from the file that don't start with '-' or '\\n'\n",
    "        with open(filepath, 'r') as file:\n",
    "            lines = [line.strip() for line in file.readlines() if not line.startswith('-') and line.strip()]\n",
    "        data = '\\t'.join(lines)\n",
    "\n",
    "        # use regular expression to extract key-value pairs separated by tabs\n",
    "        pattern = re.compile(r'([^\\t]+)\\t+([^\\t]+)')\n",
    "        matches_dict = dict(pattern.findall(data))\n",
    "\n",
    "        # add the filename as a key in the dictionary\n",
    "        matches_dict['species'] = species\n",
    "\n",
    "        # Append the dictionary to the list\n",
    "        data_list.append(matches_dict)\n",
    "    \n",
    "    # create a DataFrame from the list\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df.set_index(df.columns[-1], inplace=True)\n",
    "    df.to_csv(output_path)\n",
    "    df.to_excel(os.path.splitext(output_path)[0]+'.xlsx')\n",
    "\n",
    "    if verbose:\n",
    "        print(df.head())\n",
    "        print(f'Data compiled at {output_path}.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating stats for ['bee', 'bee_test', 'rice', 'rice_test', 'arabadop', 'arabadop_test', 'human_mane', 'human_mane_test', 'human_chess', 'human_chess_test']...\n",
      "Parsing ./output/bee/score_stats.out...\n",
      "Parsing ./output/bee_test/score_stats.out...\n",
      "Parsing ./output/rice/score_stats.out...\n",
      "Parsing ./output/rice_test/score_stats.out...\n",
      "Parsing ./output/arabadop/score_stats.out...\n",
      "Parsing ./output/arabadop_test/score_stats.out...\n",
      "Parsing ./output/human_mane/score_stats.out...\n",
      "Parsing ./output/human_mane_test/score_stats.out...\n",
      "Parsing ./output/human_chess/score_stats.out...\n",
      "Parsing ./output/human_chess_test/score_stats.out...\n",
      "          LiftOn best Liftoff best miniprot best  \\\n",
      "species                                            \n",
      "bee               548            3             0   \n",
      "bee_test          565            3             0   \n",
      "rice             1334            2            18   \n",
      "rice_test        1334            2            18   \n",
      "arabadop         3101           16            40   \n",
      "\n",
      "          LiftOn worse than Liftoff only LiftOn worse than miniprot only  \\\n",
      "species                                                                    \n",
      "bee                                    2                               0   \n",
      "bee_test                               2                               0   \n",
      "rice                                   2                              16   \n",
      "rice_test                              2                              16   \n",
      "arabadop                              16                              38   \n",
      "\n",
      "          LiftOn worst LiftOn equal to Liftoff only  \\\n",
      "species                                               \n",
      "bee                  1                         7490   \n",
      "bee_test             1                         7514   \n",
      "rice                 2                         7876   \n",
      "rice_test            2                         7876   \n",
      "arabadop             2                        10678   \n",
      "\n",
      "          LiftOn equal to miniprot only All equal Total counts  ...  \\\n",
      "species                                                         ...   \n",
      "bee                                 256     19679        27974  ...   \n",
      "bee_test                            264     19823        28167  ...   \n",
      "rice                                398     44219        53844  ...   \n",
      "rice_test                           395     44218        53840  ...   \n",
      "arabadop                           1219     38994        54038  ...   \n",
      "\n",
      "          liftoff_score>lifton_score liftoff_score=lifton_score  \\\n",
      "species                                                           \n",
      "bee                                3                      27169   \n",
      "bee_test                           3                      27337   \n",
      "rice                               4                      52095   \n",
      "rice_test                          4                      52094   \n",
      "arabadop                          18                      49672   \n",
      "\n",
      "          miniprot_score>liftoff_score miniprot_score=liftoff_score  \\\n",
      "species                                                               \n",
      "bee                                323                        19920   \n",
      "bee_test                           335                        20071   \n",
      "rice                               596                        44793   \n",
      "rice_test                          593                        44792   \n",
      "arabadop                          1758                        40618   \n",
      "\n",
      "          miniprot_score>lifton_score miniprot_score=lifton_score  \\\n",
      "species                                                             \n",
      "bee                                 1                       19935   \n",
      "bee_test                            1                       20087   \n",
      "rice                               18                       44617   \n",
      "rice_test                          18                       44613   \n",
      "arabadop                           40                       40213   \n",
      "\n",
      "          lifton_score>liftoff_score lifton_score=liftoff_score  \\\n",
      "species                                                           \n",
      "bee                              802                      27169   \n",
      "bee_test                         827                      27337   \n",
      "rice                            1745                      52095   \n",
      "rice_test                       1742                      52094   \n",
      "arabadop                        4348                      49672   \n",
      "\n",
      "          lifton_score>miniprot_score lifton_score=miniprot_score  \n",
      "species                                                            \n",
      "bee                              8038                       19935  \n",
      "bee_test                         8079                       20087  \n",
      "rice                             9209                       44617  \n",
      "rice_test                        9209                       44613  \n",
      "arabadop                        13785                       40213  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "Data compiled at ./output/agg_score_stats.csv.\n"
     ]
    }
   ],
   "source": [
    "'''========================= DRIVER ========================'''\n",
    "# collecting the scores into an output file\n",
    "aggregate_file = f'{OUTPUT_DIR}/agg_score_stats.csv'\n",
    "SPECIES = ['bee', 'bee_test', 'rice', 'rice_test', 'arabadop', 'arabadop_test', 'human_mane', 'human_mane_test', 'human_chess', 'human_chess_test']\n",
    "\n",
    "# for s in SPECIES:\n",
    "#     df = read_score(f'./results/{s}/score.txt')\n",
    "#     generate_stats(df, s, clean=True, verbose=True)\n",
    "\n",
    "aggregate_stats(SPECIES, aggregate_file, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize results with igv viewer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
