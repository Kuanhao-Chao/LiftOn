{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Tests\n",
    "for LiftOn, Liftoff, and miniprot\n",
    "- on CHM13_MANE and CHM13_RefSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to the directory `/ccb/salz2/kh.chao/Lifton/results/` to access the LiftOn results. Within this directory, you’ll find subdirectories for `bee`, `rice`, `arabadop` (arabidopsis), `human_mane`, and `human_chess`\n",
    "Each subdirectory contains a `score.txt` file with columns for “transcript ID,” “Liftoff protein sequence identity,” “miniprot protein sequence identity,” “LiftOn protein sequence identity,” “Category,” “Mutation types,” and “gene_locus.”\n",
    "Here are three things we want to check first:\n",
    "- LiftOn score is better than Liftoff & miniprot\n",
    "- How many transcripts have the same score as Liftoff; how many transcripts have the same score as miniprot (this category means that LiftOn directly uses either Liftoff or miniprot’s CDSs)\n",
    "- LiftOn score is worse than Liftoff & miniprot\n",
    "\n",
    "For 1 and 3, you can use the automated igv.sh script to visualize the results on IGV. Specifically:\n",
    "- For 1, we want to see how exactly LiftOn chains the CDSs and provides an improved annotation (and how many cases)\n",
    "- For 3, examine why LiftOn may have inaccurately annotated. This helps us to improve the current version of LiftOn\n",
    "\n",
    "The sequence identity dot plots and frequency plots are here: `/ccb/salz2/kh.chao/Lifton/results/arabadop/visualization/`.\n",
    "In the identity dot plots, you can see there are still some dots that are on the bottom right panel. We want to fix those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "OUTPUT_DIR = './output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect scores from original score files\n",
    "referencing kh's code from `ccb/salz2/kh.chao/Lifton/lifton/lifton_utils.py` -> `write_lifton_status()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_lifton_status(fw_score, transcript_id, transcript, lifton_status):\n",
    "#     final_status = \";\".join(lifton_status.status)\n",
    "#     fw_score.write(f\"{transcript_id}\\t{lifton_status.liftoff}\\t{lifton_status.miniprot}\\t{lifton_status.lifton}\\t{lifton_status.annotation}\\t{final_status}\\t{transcript.seqid}:{transcript.start}-{transcript.end}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate stats from score comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_score(score_file, verbose=False):\n",
    "    # read the given species file into a dataframe\n",
    "    column_names = ['transcript_id', 'liftoff_score', 'miniprot_score', 'lifton_score', 'category', 'mutation_type', 'gene_locus']\n",
    "    df = pd.read_csv(score_file,  names=column_names, delimiter='\\t')\n",
    "    if verbose:\n",
    "        print(f'Reading in {score_file}...')\n",
    "        print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''!!!!!!WARNING: DELETING DIRECTORIES!!!!!!!'''\n",
    "def delete_directory(directory_path):\n",
    "    if os.path.exists(directory_path):\n",
    "        # The directory exists, so delete it\n",
    "        try:\n",
    "            shutil.rmtree(directory_path)\n",
    "            print(f\"Directory '{directory_path}' deleted successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting directory '{directory_path}': {e}\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory_path}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stats(df, species, clean=False, verbose=False):\n",
    "    # get filenames\n",
    "    if verbose: \n",
    "        print(f'Collecting stats for {species}...')\n",
    "    \n",
    "    if clean:\n",
    "        delete_directory(f'{OUTPUT_DIR}/{species}/')\n",
    "        \n",
    "    os.makedirs(f'{OUTPUT_DIR}/{species}/', exist_ok=True)\n",
    "    lifton_better_filepath = f'{OUTPUT_DIR}/{species}/lifton_better.out'\n",
    "    lifton_worse_filepath = f'{OUTPUT_DIR}/{species}/lifton_worse.out'\n",
    "    stats_filepath = f'{OUTPUT_DIR}/{species}/score_stats.out'\n",
    "    \n",
    "    # collect statistics within dataframe\n",
    "    for col1 in df.columns[1:4]:\n",
    "        for col2 in df.columns[1:4]:\n",
    "            if col1 != col2:\n",
    "                is_greater = df[col1] > df[col2]\n",
    "                is_equal = df[col1] == df[col2]\n",
    "\n",
    "                df[f'{col1}>{col2}'] = is_greater\n",
    "                df[f'{col1}={col2}'] = is_equal\n",
    "    \n",
    "    stats_counts = df.iloc[:, -12:].sum()\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Collected stat counts:\\n{stats_counts}\\n')\n",
    "    \n",
    "    # write selected scenarios to file\n",
    "    out = '-------------- SCENARIOS ------------\\n'\n",
    "    ## 1. lifton best\n",
    "    lifton_best = df[df['lifton_score>liftoff_score'] & df['lifton_score>miniprot_score']].iloc[:, :-12]\n",
    "    lifton_best.to_csv(lifton_better_filepath, index=False, header=False, sep='\\t')\n",
    "    out += f'LiftOn best\\t\\t\\t\\t\\t\\t{len(lifton_best)}\\n'\n",
    "    ## 2. liftoff best\n",
    "    liftoff_best = df[df['liftoff_score>lifton_score'] & df['liftoff_score>miniprot_score']]\n",
    "    out += f'Liftoff best\\t\\t\\t\\t\\t{len(liftoff_best)}\\n'\n",
    "    ## 3. miniprot best\n",
    "    miniprot_best = df[df['miniprot_score>lifton_score'] & df['miniprot_score>liftoff_score']]\n",
    "    out += f'miniprot best\\t\\t\\t\\t\\t{len(miniprot_best)}\\n'\n",
    "    ## 4. lifton < liftoff only\n",
    "    lifton_worse_liftoff = df[df['liftoff_score>lifton_score'] & ~df['miniprot_score>lifton_score']].iloc[:, :-12]\n",
    "    lifton_worse_liftoff.to_csv(lifton_worse_filepath, index=False, header=False, sep='\\t')\n",
    "    out += f'LiftOn worse than Liftoff only\\t{len(lifton_worse_liftoff)}\\n' \n",
    "    ## 5. lifton < miniprot only\n",
    "    lifton_worse_miniprot = df[df['miniprot_score>lifton_score'] & ~df['liftoff_score>lifton_score']].iloc[:, :-12]\n",
    "    lifton_worse_miniprot.to_csv(lifton_worse_filepath, mode='a', index=False, header=False, sep='\\t')\n",
    "    out += f'LiftOn worse than miniprot only\\t{len(lifton_worse_miniprot)}\\n' \n",
    "    ## 6. lifton worst\n",
    "    lifton_worst = df[df['liftoff_score>lifton_score'] & df['miniprot_score>lifton_score']].iloc[:, :-12]\n",
    "    lifton_worst.to_csv(lifton_worse_filepath, mode='a', index=False, header=False, sep='\\t')\n",
    "    out += f'LiftOn worst\\t\\t\\t\\t\\t{len(lifton_worst)}\\n' \n",
    "    ## 7. lifton == liftoff only\n",
    "    lifton_eq_liftoff = df[df['lifton_score=liftoff_score'] & ~df['lifton_score=miniprot_score']]\n",
    "    out += f'LiftOn equal to Liftoff only\\t{len(lifton_eq_liftoff)}\\n'\n",
    "    ## 8. lifton == miniprot only \n",
    "    lifton_eq_miniprot = df[df['lifton_score=miniprot_score'] & ~df['lifton_score=liftoff_score']]\n",
    "    out += f'LiftOn equal to miniprot only\\t{len(lifton_eq_miniprot)}\\n'\n",
    "    ## 9. all equal\n",
    "    all_eq = df[df['lifton_score=miniprot_score'] & df['lifton_score=liftoff_score']]\n",
    "    out += f'All equal\\t\\t\\t\\t\\t\\t{len(all_eq)}\\n'\n",
    "    ## 10. total counted\n",
    "    out += f'Total counts\\t\\t\\t\\t\\t{len(df)}\\n'\n",
    "\n",
    "    # do all mutually exclusive cases, ranked\n",
    "    out += '\\n----------------- ALL ---------------\\n'\n",
    "    out += f\"lifton=liftoff=miniprot\\t{len(all_eq)}\\n\"\n",
    "    out += f\"lifton>liftoff=miniprot\\t{len(df[df['lifton_score>liftoff_score'] & df['liftoff_score=miniprot_score']])}\\n\"\n",
    "    out += f\"lifton>liftoff>miniprot\\t{len(df[df['lifton_score>liftoff_score'] & df['liftoff_score>miniprot_score']])}\\n\"\n",
    "    out += f\"lifton>miniprot>liftoff\\t{len(df[df['lifton_score>miniprot_score'] & df['miniprot_score>liftoff_score']])}\\n\"\n",
    "    out += f\"lifton=liftoff>miniprot\\t{len(df[df['lifton_score=liftoff_score'] & df['lifton_score>miniprot_score']])}\\n\"\n",
    "    out += f\"lifton=miniprot>liftoff\\t{len(df[df['lifton_score=miniprot_score'] & df['lifton_score>liftoff_score']])}\\n\"\n",
    "    out += f\"liftoff>lifton>miniprot\\t{len(df[df['liftoff_score>lifton_score'] & df['lifton_score>miniprot_score']])}\\n\"\n",
    "    out += f\"miniprot>lifton>liftoff\\t{len(df[df['miniprot_score>lifton_score'] & df['lifton_score>liftoff_score']])}\\n\"\n",
    "    out += f\"liftoff>lifton=miniprot\\t{len(df[df['liftoff_score>lifton_score'] & df['lifton_score=miniprot_score']])}\\n\"\n",
    "    out += f\"miniprot>lifton=liftoff\\t{len(df[df['miniprot_score>lifton_score'] & df['lifton_score=liftoff_score']])}\\n\"\n",
    "    out += f\"liftoff=miniprot>lifton\\t{len(df[df['liftoff_score=miniprot_score'] & df['liftoff_score>lifton_score']])}\\n\"\n",
    "    out += f\"liftoff>miniprot>lifton\\t{len(df[df['liftoff_score>miniprot_score'] & df['miniprot_score>lifton_score']])}\\n\"\n",
    "    out += f\"miniprot>liftoff>lifton\\t{len(df[df['miniprot_score>liftoff_score'] & df['liftoff_score>lifton_score']])}\\n\"\n",
    "    out += f\"all\\t\\t\\t\\t\\t\\t{len(df)}\\n\"\n",
    "\n",
    "    if verbose: \n",
    "        print(out)\n",
    "    \n",
    "    # adding raw data to the file\n",
    "    with open(stats_filepath, 'a') as f:\n",
    "        f.write(out)\n",
    "        f.write('\\n----------------- RAW ---------------\\n')\n",
    "    stats_counts.to_csv(stats_filepath, mode='a', header=False, sep='\\t')\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Wrote stat results to: {stats_filepath}, {lifton_better_filepath}, {lifton_worse_filepath}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_stats(species_list, output_path, verbose=False):\n",
    "\n",
    "    # read in each individual stats_file and generate an aggregate file\n",
    "    if verbose:\n",
    "        print(f'Aggregating stats for {list(species_list)}...')\n",
    "\n",
    "    data_list = []\n",
    "    for species in species_list:\n",
    "        \n",
    "        # define file\n",
    "        filepath = f'{OUTPUT_DIR}/{species}/score_stats.out'\n",
    "        if verbose:\n",
    "            print(f'Parsing {filepath}...')\n",
    "    \n",
    "        # read the lines from the file that don't start with '-' or '\\n'\n",
    "        with open(filepath, 'r') as file:\n",
    "            lines = [line.strip() for line in file.readlines() if not line.startswith('-') and line.strip()]\n",
    "        data = '\\t'.join(lines)\n",
    "\n",
    "        # use regular expression to extract key-value pairs separated by tabs\n",
    "        pattern = re.compile(r'([^\\t]+)\\t+([^\\t]+)')\n",
    "        matches_dict = dict(pattern.findall(data))\n",
    "        matches_dict['species'] = species\n",
    "\n",
    "        # add to list\n",
    "        data_list.append(matches_dict)\n",
    "    \n",
    "    # create a DataFrame from the list\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df.set_index(df.columns[-1], inplace=True)\n",
    "    df.to_csv(output_path)\n",
    "    df.to_excel(os.path.splitext(output_path)[0]+'.xlsx')\n",
    "\n",
    "    if verbose:\n",
    "        print(df.head())\n",
    "        print(f'Data compiled at {output_path}.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUTPUT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/smao10/lifton/comparison/compare.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsalz3/home/smao10/lifton/comparison/compare.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m'''========================= DRIVER ========================'''\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsalz3/home/smao10/lifton/comparison/compare.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# collecting the scores into an output file\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsalz3/home/smao10/lifton/comparison/compare.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m aggregate_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mOUTPUT_DIR\u001b[39m}\u001b[39;00m\u001b[39m/agg_score_stats.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsalz3/home/smao10/lifton/comparison/compare.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#SPECIES = ['bee', 'bee_test', 'rice', 'rice_test', 'arabadop', 'arabadop_test', 'human_mane', 'human_mane_test', 'human_chess', 'human_chess_test'] #_v1\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsalz3/home/smao10/lifton/comparison/compare.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#SPECIES = ['human_mane_test', 'human_refseq_test', 'human_to_chimp_test', 'human_mane_to_mouse_test', 'human_refseq_to_mouse_test', 'mouse_test', 'mouse_to_rat_test', \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsalz3/home/smao10/lifton/comparison/compare.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#           'bee_test', 'drosophila_test', 'drosophila_erecta_test', 'rice_test', 'yeast_test', 'arabadop_test'] #_v2\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsalz3/home/smao10/lifton/comparison/compare.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m SPECIES \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39marabadop\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbee\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdrosophila_erecta\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m#, 'human_mane', 'human_mane_to_mouse', 'human_refseq', 'human_to_chimp', 'mouse', 'rice']\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OUTPUT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "'''========================= DRIVER ========================'''\n",
    "# collecting the scores into an output file\n",
    "aggregate_file = f'{OUTPUT_DIR}/agg_score_stats.csv'\n",
    "#SPECIES = ['bee', 'bee_test', 'rice', 'rice_test', 'arabadop', 'arabadop_test', 'human_mane', 'human_mane_test', 'human_chess', 'human_chess_test'] #_v1\n",
    "#SPECIES = ['human_mane_test', 'human_refseq_test', 'human_to_chimp_test', 'human_mane_to_mouse_test', 'human_refseq_to_mouse_test', 'mouse_test', 'mouse_to_rat_test', \n",
    "#           'bee_test', 'drosophila_test', 'drosophila_erecta_test', 'rice_test', 'yeast_test', 'arabadop_test'] #_v2\n",
    "SPECIES = ['arabadop', 'bee', 'drosophila_erecta']#, 'human_mane', 'human_mane_to_mouse', 'human_refseq', 'human_to_chimp', 'mouse', 'rice']\n",
    "\n",
    "for s in SPECIES:\n",
    "    df = read_score(f'./results/{s}/lifton_output/score.txt')\n",
    "    generate_stats(df, s, clean=False, verbose=True)\n",
    "\n",
    "aggregate_stats(SPECIES, aggregate_file, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize results with igv viewer\n",
    "use the `igv.sh` in the `igv_viewer/` folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bed_from_input(filepath):\n",
    "    \n",
    "    # get input data\n",
    "    input_data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        input_data = f.readlines()\n",
    "\n",
    "    bed_lines = []\n",
    "    bed_lines.append('chr\\tstart\\tend\\tID\\tliftoff\\tminiprot\\tlifton\\tmethod\\tmutation')\n",
    "    for line in input_data:\n",
    "        # Skip empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        # Split the line into columns\n",
    "        columns = line.strip().split('\\t')\n",
    "\n",
    "        # Extract information\n",
    "        chromosome_range = columns[-1].split(':')\n",
    "        chromosome = chromosome_range[0]\n",
    "        start, end = map(int, chromosome_range[1].split('-'))\n",
    "        identifier = columns[0]\n",
    "        liftoff = columns[1]\n",
    "        miniprot = columns[2]\n",
    "        lifton = columns[3]\n",
    "        method = columns[4]\n",
    "        mutation = columns[5]\n",
    "\n",
    "        # Create BED line\n",
    "        bed_line = f\"{chromosome}\\t{start}\\t{end}\\t{identifier}\\t{liftoff}\\t{miniprot}\\t{lifton}\\t{method}\\t{mutation}\"\n",
    "        bed_lines.append(bed_line)\n",
    "\n",
    "    # Combine BED lines into a single string\n",
    "    result_bed = '\\n'.join(bed_lines)\n",
    "\n",
    "    # write file\n",
    "    root_name = os.path.splitext(filepath)[0]\n",
    "    with open(root_name+'.tsv', 'w') as f:\n",
    "        f.write(result_bed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in SPECIES:\n",
    "    # try:\n",
    "    #     os.remove(f'./output/{s}/lifton_better.tsv')\n",
    "    #     os.remove(f'./output/{s}/lifton_worse.tsv')\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    os.makedirs(f'./output/{s}/igv_images/', exist_ok=True)   \n",
    "    extract_bed_from_input(f'./output/{s}/lifton_better.out')\n",
    "    extract_bed_from_input(f'./output/{s}/lifton_worse.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "- human_refseq_test\n",
    "- mouse_to_rat_test\n",
    "\n",
    "## subplots on figure \n",
    "A. zoomed out view of liftoff, miniprot, and lifton\n",
    "    - liftoff correct, miniprot wrong, miniprot very truncated \\\\\n",
    "B. zoomed in \\\\\n",
    "C. ,, \\\\\n",
    "    - liftoff wrong, miniprot correct, liftoff very truncated \n",
    "D. zoomed in \\\\\n",
    "E. ,, \n",
    "    - both wrong, lifton chaining algorithm fixes to 1 \\\\\n",
    "F. zoomed in \\\\\n",
    "\n",
    "+ look for errors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
